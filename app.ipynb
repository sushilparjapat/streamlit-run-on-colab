{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install All packages"
      ],
      "metadata": {
        "id": "V-l7L9cGbFgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "ILokYD4rbntz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "yEegcWRTbplu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have Launching terminal \\n\n",
        "\n",
        "command 1  -- curl -fsSL https://ollama.com/install.sh | sh  \n",
        "command2 -- ollama serve & ollama run llama2"
      ],
      "metadata": {
        "id": "HtNxxWCtbs8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Streamlit app"
      ],
      "metadata": {
        "id": "SAjfb-W0cEDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.llms import Ollama\n",
        "import streamlit as st\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\"\n",
        "\n",
        "## Prompt Template\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
        "        (\"user\",\"Question:{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "## streamlit framework\n",
        "\n",
        "st.title('Langchain Demo With LLAMA2 API')\n",
        "input_text=st.text_input(\"Search the topic u want\")\n",
        "\n",
        "# ollama LLAma2 LLm\n",
        "llm=Ollama(model=\"llama2\")\n",
        "\n",
        "output_parser=StrOutputParser()\n",
        "\n",
        "chain=prompt|llm|output_parser\n",
        "\n",
        "if input_text:\n",
        "    st.write(chain.invoke({\"question\":input_text}))"
      ],
      "metadata": {
        "id": "qIWNKUJgcGR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "tD4VhIQocLS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have --- your url is: https://full-rooms-jump.loca.lt\n",
        "\n",
        "  You can now view your Streamlit app in your browser.\n",
        "\n",
        "  Local URL: http://localhost:8501\n",
        "  Network URL: http://172.28.0.12:8501\n",
        "  External URL: http://34.16.196.171:8501\n",
        "\n",
        "  step1 --click first url\n",
        "  step2 -- open tunnel   and paste it 34.16.196.171"
      ],
      "metadata": {
        "id": "k_spl3Y3cNbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VK0NdId5br6a"
      }
    }
  ]
}